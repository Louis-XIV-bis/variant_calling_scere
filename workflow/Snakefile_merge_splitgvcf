# Author: Louis OLLIVIER (louis.xiv.bis@gmail.com)
# Date : February 2023 

configfile: "config/config.yaml"

# Load the predefined resource requirments ("config/resources.yaml")
import yaml, json, numpy as np, pandas as pd
with open("config/resources.yaml", "r") as f:
    resources = yaml.safe_load(f)

conda: "workflow/envs/environment.yaml"

############# Downloading, merging and filtering the information files #############
# Download all the tsv files from the ENA IDs. They contain all the information
# requiered for the rest of the pipeline (sample ID, ftp link to dl fastq, etc).
# After that, it's merged into one unique file and filtered / processed (e.g. keep
# only S.cere sequences). Then, a csv table is created for each ENA_strain ID from the table.

# Done by the Python script ran before this pipeline, we just read the list of IDs
try:   
    with open('results/tables_merge_gvcf/ENA_strain_list.json', 'r') as file:
        ENA_strain_list = np.array(json.load(file))
except FileNotFoundError:
    raise FileNotFoundError("Please, make sure you ran the 'generate_table.py' script before that pipeline (python generate_tables.py merge_gvcf). If you did, the output file was not correctly created.")

# Chromosome ID defined in the split_gvcf script (S. cerevisiae specific)
contigs=["001133","001134","001135","001136","001137","001138","001139","001140","001141",
        "001142","001143","001144","001145","001146","001147","001148","001224"]

# Create a function to generate the expected input files
def input_files_for_contig(contig):
    return [f"results/split_gvcf/{strain}_{contig}.g.vcf.gz" for strain in ENA_strain_list] 

# Generate all the possible combinations of contigs and strains for the output
output_merge_contig_gvcf = [f"results/vcf/merged_strains_{contig}.g.vcf.gz" for contig in contigs] 
output_merge_contig_vcf = [f"results/vcf/unfiltered_merged_strains_{contig}.vcf.gz" for contig in contigs]
output_filtered_merge_contig_vcf = [f"results/vcf/filtered_merged_strains_{contig}.vcf.gz" for contig in contigs]
output_reremoved_filtered_merge_contig_vcf = [f"results/vcf/repremoved_filtered_merged_strains_{contig}.vcf.gz" for contig in contigs]


localrules:
    all,

rule all:
    input:
        output_merge_contig_gvcf, 
        output_merge_contig_vcf,
        output_filtered_merge_contig_vcf,
        output_reremoved_filtered_merge_contig_vcf,


# # Combine per sample gVCF into multi-sample gVCF using CombineGVCFs
# # (since we only produce gVCF w/ HaplotypeCaller it's OK tu use this tool).

rule merge_gvcfs:
    input:
        lambda wildcards: input_files_for_contig(wildcards.contig)
    output:
        "results/vcf/merged_strains_{contig}.g.vcf.gz"
    params:
        ref_genome=config["ref_genome"],
        file_name=lambda wildcards: f"list_gvcf_{wildcards.contig}.list"

    threads: resources["merge_gvcfs"]["cpu_tasks"]
    resources:
        slurm_partition=resources["merge_gvcfs"]["partition"],
        mem_mb=resources["merge_gvcfs"]["memory"],
        tasks=resources["merge_gvcfs"]["tasks"],
        cpus_per_task=resources["merge_gvcfs"]["cpu_tasks"],
        jobname=resources["merge_gvcfs"]["jobname"],
    log:
        stdout="logs/merge_gvcfs_{contig}.stdout",
        stderr="logs/merge_gvcfs_{contig}.stderr"    
    run:
        with open(params.file_name, "w") as file:
            for element in input:
                file.write(str(element) + "\n")

        shell(
            "gatk CombineGVCFs -R {params.ref_genome} -O {output} -V {params.file_name} > {log.stdout} 2> {log.stderr}"
        )
        os.remove(params.file_name)

# Convert the multi-sample gVCF to the multi-sample VCF file
# using GenotypeGVCFs.


rule gvcf_to_vcf:
    input:
        "results/vcf/merged_strains_{contig}.g.vcf.gz"
    output:
        "results/vcf/unfiltered_merged_strains_{contig}.vcf.gz"

    params:
        ref_genome=config["ref_genome"],
    threads: resources["gvcf_to_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["gvcf_to_vcf"]["partition"],
        mem_mb=resources["gvcf_to_vcf"]["memory"],
        tasks=resources["gvcf_to_vcf"]["tasks"],
        cpus_per_task=resources["gvcf_to_vcf"]["cpu_tasks"],
        jobname=resources["gvcf_to_vcf"]["jobname"],
    log:
        stdout="logs/gvcf_to_vcf_{contig}.stdout", stderr="logs/gvcf_to_vcf_{contig}.stderr"
    shell:
        "gatk GenotypeGVCFs -R {params.ref_genome} -V {input} -O {output} > {log.stdout} 2> {log.stderr}"


# Fill the filter column in the VCF: hard filtering based on previous tests (remove if TRUE)


rule add_filter_vcf:
    input:
        "results/vcf/unfiltered_merged_strains_{contig}.vcf.gz"
    output:
        temp("results/vcf/addfilter_merged_strains_{contig}.vcf.gz"),
    params:
        ref_genome=config["ref_genome"],
    threads: resources["add_filter_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["add_filter_vcf"]["partition"],
        mem_mb=resources["add_filter_vcf"]["memory"],
        tasks=resources["add_filter_vcf"]["tasks"],
        cpus_per_task=resources["add_filter_vcf"]["cpu_tasks"],
        jobname=resources["add_filter_vcf"]["jobname"],
    log:
        stdout="logs/add_filter_vcf_{contig}.stdout", stderr="logs/add_filter_vcf_{contig}.stderr"
    shell:
        """
        gatk VariantFiltration -R {params.ref_genome} \
        -V {input} \
        -filter "QD < 10.0" --filter-name "QD10" \
        -filter "SOR > 3.0" --filter-name "SOR3" \
        -filter "FS > 60.0" --filter-name "FS60" \
        -filter "MQ < 50.0" --filter-name "MQ50" \
        -O {output} > {log.stdout} 2> {log.stderr}
        """

#    "-filter 'MQRankSum < (-12.5)' --filter-name 'MQRankSum-12.5' "
#    "-filter 'ReadPosRankSum < (-8)' --filter-name ReadPosRankSum-8' "

# Remove the SNP that didn't pass the filter (based on the FILTER column)


rule filter_vcf:
    input:
        "results/vcf/addfilter_merged_strains_{contig}.vcf.gz"    
    
    output:
        "results/vcf/filtered_merged_strains_{contig}.vcf.gz",
        
    threads: resources["filter_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["filter_vcf"]["partition"],
        mem_mb=resources["filter_vcf"]["memory"],
        tasks=resources["filter_vcf"]["tasks"],
        cpus_per_task=resources["filter_vcf"]["cpu_tasks"],
        jobname=resources["filter_vcf"]["jobname"],
    log:
        stderr="logs/filter_vcf_{contig}.stderr"
    shell:
        "vcftools --gzvcf {input} --remove-filtered-all --recode --stdout  gzip -c > {output} 2> {log.stderr}"

############# Remove repeted regions #############
# Remove repeted regions from the genome (see resources/README.md) 
# for more info about the input file

rule remove_rep_regions:
    priority: 12
    input:
        "results/vcf/filtered_merged_strains_{contig}.vcf.gz",
    output:
        "results/vcf/repremoved_filtered_merged_strains_{contig}.vcf.gz",
    params:
        rep_regions=config["rep_regions"],
    threads: resources["remove_rep_regions"]["cpu_tasks"]
    resources:
        slurm_partition=resources["remove_rep_regions"]["partition"],
        mem_mb=resources["remove_rep_regions"]["memory"],
        tasks=resources["remove_rep_regions"]["tasks"],
        cpus_per_task=resources["remove_rep_regions"]["cpu_tasks"],
        jobname=resources["remove_rep_regions"]["jobname"],
    log:
        stderr="logs/remove_rep_regions_{contig}.stderr"
    shell:
        """
        bcftools view -h {input} > temp_{wildcards.contig}.vcf 2> {log.stderr}; \ 
        bedtools subtract -a {input} -b {params.rep_regions} >> temp_{wildcards.contig}.vcf 2>> {log.stderr}; \
        tabix -p vcf temp_{wildcards.contig}.vcf; \ 
        bgzip -c temp_{wildcards.contig}.vcf > {output}; \ 
        rm temp_{wildcards.contig}.vcf
        """